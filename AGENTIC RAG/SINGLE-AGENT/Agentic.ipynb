{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f24cad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq  \n",
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113a8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of PDF Pages 968\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH='Banking_data/'\n",
    "def load_pdf_files(data):\n",
    "    loader=DirectoryLoader(data,\n",
    "                           glob='*.pdf',\n",
    "                           loader_cls=PyPDFLoader)\n",
    "    documents=loader.load()\n",
    "    return documents\n",
    "\n",
    "documents=load_pdf_files(data=DATA_PATH)\n",
    "print('Length of PDF Pages', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e596e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 3134\n"
     ]
    }
   ],
   "source": [
    "def create_chunks(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks=create_chunks(extracted_data=documents)\n",
    "print('Length of Text Chunks', len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d3f38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model():\n",
    "    embedding_model=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embedding_model\n",
    "\n",
    "embedding_model=get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec08bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB_FAISS_PATH='vectorstore/db_faiss'\n",
    "# vectorstore=FAISS.from_documents(text_chunks, embedding_model)\n",
    "# vectorstore.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c21e17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "851f80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "def search(text: str, top_k: int):\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    keyword_retriever = BM25Retriever.from_documents(text_chunks)\n",
    "    keyword_retriever.k = top_k\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[retriever, keyword_retriever],\n",
    "        weights=[0.5, 0.5]\n",
    "    )    \n",
    "    return ensemble_retriever.get_relevant_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_system_prompt = \"\"\"Your job is to decide if a given question can be answered with the given context. \n",
    "If context can answer the question return 1.\n",
    "If not return 0.\n",
    "\n",
    "Do not return anything else except 0 or 1.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc5e5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert for answering questions. Answer the question according only to the given context.\n",
    "If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "Your answer MUST be informative, concise, and action driven. Your response must be in Markdown.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a67dcb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is Risk Governance Framework?\"\n",
    "results = search(question, top_k=5)\n",
    "context = format_docs(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "GROQ_API_KEY = \"your_groq_api_key_here\" \n",
    "\n",
    "def load_llm():\n",
    "    llm = ChatGroq(\n",
    "        api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama3-70b-8192\", \n",
    "        temperature=0.9,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "# Example usage\n",
    "llm = load_llm()\n",
    "\n",
    "response = llm.invoke([\n",
    "    {\"role\": \"system\", \"content\": decision_system_prompt.format(question=question, context=context)},\n",
    "    {\"role\": \"user\", \"content\": \"Decide now\"}\n",
    "])\n",
    "\n",
    "# Extract modelâ€™s text\n",
    "has_answer = response.content\n",
    "print(has_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_3688\\3114448315.py:20: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  results = DDGS().text(question, max_results=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def format_ddg_results(results):\n",
    "#     formatted = []\n",
    "#     for i, doc in enumerate(results, start=1):\n",
    "#         formatted.append(f\"--- Result {i} ---\\n{doc.page_content}\\n{doc.metadata}\\n\")\n",
    "#     return \"\\n\".join(formatted)\n",
    "\n",
    "# formatted_output = format_ddg_results(results)\n",
    "\n",
    "# print(formatted_output)\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "def format_ddg_results(results):\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(results, start=1):\n",
    "        title = doc.get(\"title\", \"No title\")\n",
    "        url = doc.get(\"href\", \"No URL\")\n",
    "        body = doc.get(\"body\", \"No description\")\n",
    "        formatted.append(f\"--- Result {i} ---\\nTitle: {title}\\nURL: {url}\\nSnippet: {body}\\n\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "results = DDGS().text(question, max_results=5) \n",
    "\n",
    "formatted_output = format_ddg_results(results)\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context can answer the question\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Risk Governance Framework**\n",
       "\n",
       "The Risk Governance Framework is a framework through which the Board and management establish and make decisions about the bank's strategy and approach to risk management; articulate and monitor adherence to the risk appetite and risk limits relative to the bank's strategy; and identify, measure, manage and control risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "if has_answer == '1':\n",
    "    print(\"Context can answer the question\")\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt.format(context=context)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(question=question)}\n",
    "    ])\n",
    "    print(\"Answer:\")\n",
    "    display(Markdown(response.content))\n",
    "\n",
    "else:\n",
    "    print(\"Context is NOT relevant. Searching online...\")\n",
    "    results = DDGS().text(question, max_results=5)\n",
    "    context = format_ddg_results(results)\n",
    "    print(\"Found online sources. Generating the response...\")\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt.format(context=context)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(question=question)}\n",
    "    ]) \n",
    "    print(\"Answer:\")\n",
    "    display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10b6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457ef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fea746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d7e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13665488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e80316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3c1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45151595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2111e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797a4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a88c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee003d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03334714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146afdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636aa64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf190f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40d75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
